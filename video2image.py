"""
@Project ï¼šCVTools
@File ï¼švideo2image.py
@Author ï¼šHaruka
@Date ï¼š2025/8/16 16:45
"""
import os
import sys
import time

import cv2
import json
import argparse
import numpy as np
import multiprocessing
from tqdm import tqdm

from perspective_transformation import PerspectiveTransformer

video_extensions = {'.mp4', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.webm', '.dav'}

# å®¹é”™é˜ˆå€¼ï¼šå…è®¸è¿ç»­ç”±å¤šå°‘å¸§è¯»å–å¤±è´¥ï¼ˆç›‘æ§è§†é¢‘åå¸§å¸¸è§ï¼Œå»ºè®®è®¾å¤§ä¸€ç‚¹ï¼Œæ¯”å¦‚100ï¼‰
MAX_TOLERANCE = 100


def pool_init(lock):
    """
    è¿›ç¨‹æ± åˆå§‹åŒ–å‡½æ•°ï¼š
    åœ¨æ¯ä¸ªå­è¿›ç¨‹å¯åŠ¨æ—¶è¿è¡Œï¼Œå°†ä¸»è¿›ç¨‹çš„é”æ³¨å†Œç»™ tqdmï¼Œ
    ç¡®ä¿æ‰€æœ‰è¿›ç¨‹ä½¿ç”¨åŒä¸€ä¸ªé”æ¥ç®¡ç†æ§åˆ¶å°è¾“å‡ºã€‚
    """
    tqdm.set_lock(lock)


def select_four_points(image: np.ndarray, title: str) -> np.ndarray | None:
    # å·¦é”®æ·»åŠ ç‚¹ï¼›`r` é‡ç½®ï¼›`Enter` ç¡®è®¤ï¼›`Esc/q` å–æ¶ˆ
    points: list[tuple[int, int]] = []

    def mouse_cb(event, x, y, flags, param):
        if event == cv2.EVENT_LBUTTONDOWN and len(points) < 4:
            points.append((x, y))

    cv2.namedWindow(title, cv2.WINDOW_NORMAL)
    cv2.setMouseCallback(title, mouse_cb)

    while True:
        vis = image.copy()
        for i, (x, y) in enumerate(points):
            cv2.circle(vis, (x, y), 5, (0, 255, 0), -1)
            cv2.putText(vis, str(i + 1), (x + 6, y - 6),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        cv2.putText(vis, "LeftClick:add | r:reset | Enter:confirm | Esc/q:cancel",
                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 200, 255), 2)
        cv2.imshow(title, vis)
        key = cv2.waitKey(20) & 0xFF
        if key in (27, ord('q')):  # Esc/q
            cv2.destroyWindow(title)
            return None
        if key == ord('r'):
            points.clear()
        if key in (13, 10) and len(points) == 4:  # Enter
            cv2.destroyWindow(title)
            return np.array(points, dtype=np.float32)


def get_first_frame(video_path: str) -> np.ndarray | None:
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return None
    ret, frame = cap.read()
    cap.release()
    return frame if ret else None


def prepare_perspective_for_video(video_path: str, cache: dict,
                                  output_size: tuple[int, int] | None = None) -> dict | None:
    video_key = os.path.basename(video_path)
    if video_key in cache:
        return cache[video_key]

    frame0 = get_first_frame(video_path)
    if frame0 is None:
        # ä½¿ç”¨ tqdm.write é¿å…æ‰“æ–­è¿›åº¦æ¡
        tqdm.write(f"âš ï¸ æ— æ³•è¯»å–é¦–å¸§: {video_path}")
        return None

    pts = select_four_points(frame0, f"Select 4 points - {video_key}")
    if pts is None:
        tqdm.write(f"âš ï¸ è·³è¿‡è§†é¢‘(æœªé€‰æ‹©ç‚¹): {video_path}")
        return None

    transformer = PerspectiveTransformer(points=pts, dst_size=output_size)
    rect = transformer.src_rect
    W, H = transformer.dst_w, transformer.dst_h
    data = {
        "src_points": rect.tolist(),
        "width": W,
        "height": H
    }
    cache[video_key] = data
    return data


def extract_frames(video_path: str, output_dir: str, video_name: str, persp_cfg: dict | None = None,
                   worker_id: int = 0):
    time.sleep(worker_id * 0.1)

    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        tqdm.write(f"âŒ æ— æ³•æ‰“å¼€: {video_name}")
        return

    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps_val = cap.get(cv2.CAP_PROP_FPS)
    fps = int(fps_val) if fps_val and fps_val > 0 else 1

    transformer = None
    if persp_cfg:
        src_points = np.array(persp_cfg["src_points"], dtype=np.float32)
        dst_size = (persp_cfg["width"], persp_cfg["height"])
        transformer = PerspectiveTransformer(
            points=src_points,
            dst_size=dst_size,
            interpolation=cv2.INTER_LINEAR,
            border_mode=cv2.BORDER_REPLICATE
        )

    short_name = os.path.basename(video_path)
    if len(short_name) > 15:
        display_name = f"{short_name[:3]}..{short_name[-10:]}"
    else:
        display_name = short_name

    desc_str = f"W-{worker_id} {display_name}"
    image_count = 0

    consecutive_errors = 0  # å½“å‰è¿ç»­é”™è¯¯è®¡æ•°

    for i in tqdm(range(frame_count), desc=desc_str, position=worker_id, leave=True, mininterval=0.5):
        ret, frame = cap.read()

        if not ret:
            consecutive_errors += 1
            if consecutive_errors > MAX_TOLERANCE:
                # è¿ç»­åå¤ªå¤šå¸§ï¼Œåˆ¤å®šä¸ºè§†é¢‘çœŸæ­£ç»“æŸ
                tqdm.write(f"âŒ {video_name} ç»“æŸäºå¸§ {i} (è¿ç»­é”™è¯¯)")
                break

            # åªæ˜¯å¶å°”åå¸§ï¼Œè·³è¿‡ï¼Œä¸ä¿å­˜å›¾ç‰‡ï¼Œç»§ç»­å¾ªç¯æ‰¾ä¸‹ä¸€å¸§
            continue

        # å¦‚æœæˆåŠŸè¯»åˆ°å¸§ï¼Œé‡ç½®é”™è¯¯è®¡æ•°å™¨
        consecutive_errors = 0
        if i % fps != 0:
            continue

        if transformer:
            frame = transformer([frame])[0]

        image_count += 1
        frame_filename = os.path.join(output_dir, f"frame_{video_name}_{image_count:04d}.jpg")
        cv2.imwrite(frame_filename, frame)

    cap.release()


def process_videos(video_dir: str, output_dir: str, enable_perspective: bool = False,
                   output_size: tuple[int, int] | None = None):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    video_files = [f for f in os.listdir(video_dir)
                   if (os.path.splitext(f)[1].lower() in video_extensions and not f.startswith('.'))]
    video_files.sort()

    have_processed = [f for f in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, f))]

    tqdm_lock = multiprocessing.RLock()

    if enable_perspective:
        # ä¸»è¿›ç¨‹äº¤äº’å–ç‚¹ï¼›æ¯å¾—åˆ°ä¸€æ®µè§†é¢‘çš„å‚æ•°ï¼Œç«‹åˆ»æŠŠå¤„ç†ä»»åŠ¡ä¸¢åˆ°åå°è¿›ç¨‹æ± 
        cache_path = os.path.join(output_dir, "points_cache.json")
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r", encoding="utf-8") as fr:
                    points_cache = json.load(fr)
            except Exception:
                points_cache = {}
        else:
            points_cache = {}
        pool = multiprocessing.Pool(
            initializer=pool_init,
            initargs=(tqdm_lock,)
        )

        try:
            worker_id = 0
            for vf in video_files:
                if os.path.splitext(vf)[0] in have_processed:
                    print(f"è·³è¿‡å·²å¤„ç†ï¼š{vf}")
                    continue
                video_path = os.path.join(video_dir, vf)
                cfg = prepare_perspective_for_video(video_path, points_cache, output_size)
                if cfg is None:
                    continue
                output_subdir = os.path.join(output_dir, os.path.splitext(vf)[0])

                pool.apply_async(extract_frames,
                                 args=(video_path, output_subdir, os.path.splitext(vf)[0], cfg, worker_id))
                worker_id += 1

            # ç‚¹é€‰å…¨éƒ¨å®Œæˆåå†å†™ç¼“å­˜å¹¶ç­‰å¾…åå°ä»»åŠ¡æ”¶å°¾
            try:
                with open(cache_path, "w", encoding="utf-8") as fw:
                    json.dump(points_cache, fw, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"âš ï¸ å†™å…¥ç¼“å­˜å¤±è´¥: {e}")

        except Exception as e:
            print(f"âŒ ä»»åŠ¡å¤„ç†å‡ºé”™ -> {e}")

        print("\n>>>ğŸš€ æ‰€æœ‰ç‚¹é€‰å®Œæˆï¼Œåå°å¤„ç†ä¸­... (è¯·å‹¿å…³é—­çª—å£)\n")
        pool.close()
        pool.join()
        print("\næ‰€æœ‰å¤„ç†å·²å®Œæˆã€‚")

    else:
        pool = multiprocessing.Pool(
            initializer=pool_init,
            initargs=(tqdm_lock,)
        )

        for i, video_file in enumerate(video_files):
            video_path = os.path.join(video_dir, video_file)
            output_subdir = os.path.join(output_dir, os.path.splitext(video_file)[0])
            pool.apply_async(extract_frames,
                             args=(video_path, output_subdir, os.path.splitext(video_file)[0], None, i))

        pool.close()
        pool.join()


def parse_args():
    parser = argparse.ArgumentParser(description="Extract 1 FPS frames, optional perspective warp.")
    parser.add_argument("--video-dir", type=str, required=True,
                        help="åŒ…å«è§†é¢‘çš„ç›®å½•è·¯å¾„")
    parser.add_argument("--output-dir", type=str, required=True,
                        help="ä¿å­˜å¸§å›¾åƒçš„ç›®å½•è·¯å¾„")
    parser.add_argument("--perspective", action="store_true",
                        default=False,
                        help="å¼€å¯é¦–å¸§é€‰ç‚¹å¹¶å¯¹æ•´æ®µè§†é¢‘åšé€è§†å˜æ¢")
    parser.add_argument("-oz", "--output-size", type=int, nargs=2, metavar=('WIDTH', 'HEIGHT'),
                        help="æŒ‡å®šé€è§†å˜æ¢åçš„è¾“å‡ºå›¾åƒå°ºå¯¸ (å®½ é«˜)ï¼Œä¾‹å¦‚: --output-size 1920 1080")

    return parser.parse_args()


if __name__ == "__main__":
    multiprocessing.set_start_method("spawn", force=True)
    args = parse_args()

    if args.output_size:
        args.perspective = True

    if os.name == 'nt':
        if any('\u4e00' <= ch <= '\u9fff' for ch in args.output_dir):
            sys.stderr.write('âš ï¸ è­¦å‘Šï¼šè¾“å‡ºç›®å½•åŒ…å«ä¸­æ–‡ï¼Œå»ºè®®ä½¿ç”¨è‹±æ–‡è·¯å¾„\n')

    process_videos(args.video_dir, args.output_dir,
                   enable_perspective=args.perspective,
                   output_size=tuple(args.output_size) if args.output_size else None)

    print("\nFrame extraction completed.")
